{
	"cells": [
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Homework 1\n",
				"\n",
				"**The following notebook is meant to help you work through Problems 2, 3, and 4 on Homework 1. You are by no means required to use it, nor are you required to fill out/use any of the boilerplate code/functions. You are welcome to implement the functions however you wish.**"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Loading data; no changes needed\n",
				"\n",
				"import numpy as np\n",
				"import matplotlib.pyplot as plt\n",
				"from cmath import exp\n",
				"train_data = np.genfromtxt(\"data/earth_temperature_sampled_train.csv\", delimiter = ',')\n",
				"year_train = train_data[:, 0] / 1000\n",
				"temp_train = train_data[:, 1]\n",
				"test_data = np.genfromtxt(\"data/earth_temperature_sampled_test.csv\", delimiter = ',')\n",
				"year_test = test_data[:, 0] / 1000\n",
				"temp_test = test_data[:, 1]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# These are test functions for your implementations in Questions 2.1 and 3.1; no changes needed\n",
				"\n",
				"def test_p2(kernel_regressor):\n",
				"    \"\"\"\n",
				"    Run this only after you have implemented the function compute_loss, which returns the loss for a tau.\n",
				"    Note that the test cases this Autograder uses are distinct from the tau values specified in the homework.\n",
				"    \"\"\"\n",
				"\n",
				"    tau1, y1 = 3, [1.57, -5.04, -4.98924104, -2.74994907, -8.88]\n",
				"    tau2, y2 = 90, [ 1.228045, -5.16138536, -4.16187715, -2.83729799, -8.31847509]\n",
				"    tau3, y3 = 2700, [-3.46763865, -5.71861367, -4.87566622, -5.56020686, -5.18940151]\n",
				"\n",
				"    test_pts = np.array([400, 500, 600, 700, 800])\n",
				"    train_data = np.genfromtxt(\"data/earth_temperature_sampled_train.csv\", delimiter = ',')[1:]\n",
				"    year_train = train_data[:, 0] / 1000\n",
				"    temp_train = train_data[:, 1]\n",
				"\n",
				"    for tau, y in zip([tau1, tau2, tau3], [y1, y2, y3]):\n",
				"        assert np.allclose(y, kernel_regressor(test_pts, tau, year_train, temp_train)), f\"Failed for tau={tau}\"\n",
				"    \n",
				"    print(\"Passed\")\n",
				"\n",
				"def test_p3(predict_knn):\n",
				"    \"\"\"\n",
				"    Run this only after you have implemented the functions predict_kernel and predict_knn.\n",
				"    \"\"\"\n",
				"    k1, y1 = 1, [1.57, -5.04, -4.99, -2.75, -8.88]\n",
				"    k2, y2 = 3, [0.37333333, -5.19, -4.13, -2.63333333, -4.5]\n",
				"    k3, y3 = 55, [-5.22981818, -5.22981818, -5.22981818, -5.41981818, -5.41981818]\n",
				"\n",
				"\n",
				"    test_pts = np.array([400, 500, 600, 700, 800])\n",
				"    train_data = np.genfromtxt(\"data/earth_temperature_sampled_train.csv\", delimiter = ',')[1:]\n",
				"    year_train = train_data[:, 0] / 1000\n",
				"    temp_train = train_data[:, 1]\n",
				"\n",
				"    for k, y in zip([k1, k2, k3], [y1, y2, y3]):\n",
				"        assert np.allclose(y, predict_knn(test_pts, k, year_train, temp_train)), f\"Failed for k={k}\"\n",
				"\n",
				"    print(\"Passed\")"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Problem 2"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Problem 2 Subpart 1"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"def kernel_regressor(x_new, tau, x_train, y_train):\n",
				"    \"\"\"\n",
				"    Run f_tau(x) with parameter tau on every entry of x_array.\n",
				"\n",
				"    :param x_array: a numpy array of x_values on which to do prediction. Shape is (n,)\n",
				"    :param float tau: lengthscale parameter\n",
				"    :param x_train: the x coordinates of the training set\n",
				"    :param y_train: the y coordinates of the training set\n",
				"    :return: if x_array = [x_1, x_2, ...], then return [f(x_1), f(x_2), ...]\n",
				"             where f is calculated wrt to the training data and tau\n",
				"    \"\"\"\n",
				"    #TODO: Implement this function\n",
				"    pass\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"test_p2(kernel_regressor)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# plot functions\n",
				"x_array = np.arange(400, 800 + 1, 1)\n",
				"for tau in [1, 50, 2500]:\n",
				"    plt.plot(x_array, kernel_regressor(x_array, tau, year_train, temp_train), label = f\"$\\\\tau = {tau}$\")\n",
				"plt.scatter(year_train, temp_train, label = \"training data\", color = \"red\")\n",
				"plt.legend()\n",
				"plt.xticks(np.arange(400, 800 + 100, 100))\n",
				"plt.ylabel(\"Temperature\")\n",
				"plt.xlabel(\"Year BCE (in thousands)\")\n",
				"plt.ylim([-10,2.5])\n",
				"\n",
				"plt.gca().invert_xaxis()\n",
				"# figure should be in your directory now, with name p1.2.png\n",
				"plt.savefig(\"images/p1.2.png\", bbox_inches = \"tight\")\n",
				"plt.show()"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Problem 2 Subpart 4"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def model_mse(predictions, true):\n",
				"    \"\"\"\n",
				"    Calculate the MSE for the given model predictions, with respect to the true values\n",
				"\n",
				"    :param predictions: predictions given by the model\n",
				"    :param true: corresponding true values\n",
				"    :return: the mean squared error\n",
				"    \"\"\"\n",
				"\n",
				"    #TODO: Implement this function\n",
				"    pass"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"for tau in [1, 50, 2500]:\n",
				"    print(f\"tau = {tau}: loss = {model_mse(kernel_regressor(year_test, tau, year_train, temp_train), temp_test)}\")"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Problem 3"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Problem 3 Subpart 1"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# This function has been implemented for you.\n",
				"\n",
				"def predict_knn(x_new, k, x_train, y_train):\n",
				"    \"\"\"\n",
				"    Returns predictions for the values in x_test, using KNN predictor with the specified k.\n",
				"\n",
				"    :param x_new: a numpy array of x_values on which to do prediction. Shape is (n,)\n",
				"    :param k: number of nearest neighbors to consider\n",
				"    :param x_train: x coordinates of training dataset\n",
				"    :param y_train: y coordinates of training dataset\n",
				"\n",
				"    :return: if x_array = [x_1, x_2, ...], then return [f(x_1), f(x_2), ...]\n",
				"             where f is the kNN with specified parameters and training set\n",
				"    \"\"\"\n",
				"    # weight matrix\n",
				"    dists = np.exp(- (x_train - x_new.reshape(-1, 1)) ** 2 / 2500)\n",
				"    # dividing by 2500 needed due to some stability issues at high distances\n",
				"\n",
				"    # argsort the rows\n",
				"    ix = dists.argsort(axis = 1)\n",
				"    ix = ix[:, -k:] # take only the k smallest distances\n",
				"    y = y_train[ix]\n",
				"\n",
				"    # sum each row\n",
				"    return np.mean(y, axis = 1)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"test_p3(predict_knn)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# plot functions\n",
				"N = year_train.shape[0]\n",
				"x_array = np.arange(400, 800, 1)\n",
				"plt.plot(x_array, predict_knn(x_array, 1, year_train, temp_train), label = \"$k = 1$\")\n",
				"plt.plot(x_array, predict_knn(x_array, 3, year_train, temp_train), label = \"$k = 3$\")\n",
				"plt.plot(x_array, predict_knn(x_array, N - 1, year_train, temp_train), label = \"$k = N - 1$\")\n",
				"plt.scatter(year_train, temp_train, label = \"training data\", color = \"red\")\n",
				"plt.ylabel(\"Temperature\")\n",
				"plt.xlabel(\"Year BCE (in thousands)\")\n",
				"\n",
				"plt.legend()\n",
				"plt.xticks(np.arange(400, 900, 100))\n",
				"plt.ylim([-10,2.5])\n",
				"\n",
				"plt.gca().invert_xaxis()\n",
				"# figure should be in your directory now, with name p2.1.png\n",
				"plt.savefig(\"images/p2.1.png\", bbox_inches = \"tight\")\n",
				"plt.show()"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Problem 3 Subpart 2"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# choose your value of k and calculate the loss\n",
				"for k in [1, 3, 55, 56]:\n",
				"    print(f\"k = {k}: MSE loss = {model_mse(predict_knn(year_test, k, year_train, temp_train), temp_test)}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"year_train.shape"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Problem 4"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Problem 4 Subpart 1"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def exp_kernel(x,mu):\n",
				"    return np.exp(-1/float(5)*np.power(x-mu,2))\n",
				"\n",
				"def f_scale(X, part = \"a\"):\n",
				"  if part == \"a\":\n",
				"    X = X/181 # 181000\n",
				"  elif part == \"b\":\n",
				"    X = X/4e2 # 4e5\n",
				"  elif part == \"c\":\n",
				"    X = X/1.81 # 1810    \n",
				"  elif part == \"d\":\n",
				"    X = X/.181 # 181\n",
				"  return X\n",
				"\n",
				"# TODO: Complete this `make_basis` function according to the above\n",
				"# specifications. The function should return the array `phi(X)`\n",
				"\n",
				"def make_basis(X,part='a'):\n",
				"  \"\"\"\n",
				"  Args:\n",
				"    X: input of years (or any variable you want to turn into the appropriate basis) as\n",
				"      ndarray with length `N`.\n",
				"    part: one of `a`, `b`, `c`, `d` depending on the basis function.\n",
				"\n",
				"  Returns:\n",
				"    ndarray `phi(X)` of shape `(N,D)`. For each part the shapes of your\n",
				"    training data `make_basis(years_train)` should be\n",
				"      (a) 57x10, (b) 57x10, (c) 57x10, (d) 57x50.\n",
				"  \"\"\"\n",
				"  \n",
				"  phi_X = [np.ones(X.shape).T]\n",
				"\n",
				"  ### DO NOT CHANGE THIS SECTION \n",
				"  ### it is to prevent numerical instability from taking the exponents of\n",
				"  ### the years, as well as break symmetry when dealing with a Fourier basis.\n",
				"  X = f_scale(X, part)\n",
				"  ### end section\n",
				"\n",
				"  # Part a) has been provided for you as an example.\n",
				"  if part == 'a':\n",
				"    for j in range(1,10):\n",
				"      phi_X.append(X**j)\n",
				"    pass\n",
				"  \n",
				"  elif part=='b':\n",
				"    pass\n",
				"\n",
				"  elif part=='c':\n",
				"    pass\n",
				"\n",
				"  elif part=='d':\n",
				"    pass\n",
				"\n",
				"  return np.vstack(phi_X).T "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"We are now solving the multi-dimensional OLS regression problem. For each $i=1,\\ldots, N$, we have \n",
				"$$ \\hat y_i = \\mathbf{w}^\\top\\mathbf{\\phi}(x_i) = \\sum_{j=1}^D w_j \\phi_j(x_i).  $$\n",
				"\n",
				"We can find the weights that minimize the MSE $\\frac 1N\\| \\mathbf{y} - \\mathbf{\\phi}(\\mathbf{X})\\mathbf{w}\\| $ with the analytic solution described in the textbook at Derivation 2.6.1.\n",
				"$$ \\mathbf{w^*} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}. $$"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Find the regression weights using the Moore-Penrose pseudoinverse.\n",
				"def find_weights(X,y):\n",
				"    w_star = np.dot(np.linalg.pinv(np.dot(X.T, X)), np.dot(X.T, y))\n",
				"    return w_star"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"_, ax = plt.subplots(2,2, figsize = (16,10))\n",
				"\n",
				"for i, part in enumerate(['a', 'b', 'c' ,'d']):\n",
				"  # Plotting the original data\n",
				"  phi_years_train = make_basis(year_train, part)\n",
				"  w = find_weights(phi_years_train, temp_train)\n",
				"\n",
				"  \n",
				"  ax[i//2, i%2].scatter(year_train, temp_train, label = \"Original Data\")\n",
				"  \n",
				"  xs = np.linspace(year_train.min(), year_train.max(), 1000)\n",
				"  y_pred = [0 for _ in xs]\n",
				"  ax[i//2, i%2].plot(xs, y_pred, color = 'orange', label = \"Basis Regression\")\n",
				"  ax[i//2, i%2].set_xlabel(\"Year\")\n",
				"  ax[i//2, i%2].set_ylabel(\"Temperature\")\n",
				"  ax[i//2, i%2].set_title(f\"OLS Basis Regression; Temperature on Years ({part})\")\n",
				"\n",
				"  ax[i//2, i%2].legend()\n",
				"\n",
				"  # TODO: Plot the regression line generated by your model. \n",
				"  ax[i//2, i%2].invert_xaxis()\n",
				"  \n",
				"plt.savefig(\"images/p3.1.png\")\n",
				"  \n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Problem 4 Subpart 2"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def mean_squared_error(X,y, w):\n",
				"  # TODO: Given a linear regression model with parameter w, compute and return the\n",
				"  # mean squared error.\n",
				"  pass\n",
				"\n",
				"def negative_log_likelihood(X,y,w, sigma):\n",
				"  # TODO: Given a probabilistic linear regression model y = w^T x + e, where\n",
				"  # e is N(0, sigma), return the negative log likelihood of the data X,y.\n",
				"  pass\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"for part in ['a', 'b', 'c', 'd']:\n",
				"  # TODO: compute the MSE and Likelihood and Print the results\n",
				"  train_mse = 181\n",
				"  test_mse = 181\n",
				"  \n",
				"  print(f\"\\nPart ({part});\\n\\n Train MSE: {train_mse:.2f}; Test MSE: {test_mse:.2f}\\n\")\n",
				"  \n",
				"  # TODO: compute the likelihood. \n",
				"  train_log_nll = 181\n",
				"  test_log_nll = 181\n",
				"  print(f\" Train Negative Log-Likelihood: {train_log_nll:.3f}; Test Negative Log-Likelihood: {test_log_nll:.3f}\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3.8.2 64-bit",
			"metadata": {
				"interpreter": {
					"hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
				}
			},
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.3"
		},
		"orig_nbformat": 4,
		"vscode": {
			"interpreter": {
				"hash": "7b3f4362230c3bba7e46b19b0ec7f9b3b5acfad1b72d9ee2617d6bdd802281bb"
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
