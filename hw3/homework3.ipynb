{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 181 HW3 Problem 3**\n",
    "---\n",
    "\n",
    "This problem serves as an introduction to building and training machine learning models with PyTorch. This notebook seems long, so here is a breakdown:\n",
    "- **0) Why PyTorch?** Explains what PyTorch is.\n",
    "- **1) Initializing tensors**  Explains how we initialize tensors.\n",
    "- **2) Gradients**  Explains how gradients work.\n",
    "- **3) MNIST**  Introduces you to a very famous toy dataset.\n",
    "- **4) Creating a neural network**  Here you create the parameters and architecture of your models from scratch.\n",
    "- **5) Training a neural network**  Here you create a training loop from scratch.\n",
    "\n",
    "Parts **0-3** are an introdution to PyTorch. The only deliverables you need to complete are clearly labelled in pars **4-5**. This suffices to replacing `'not implemented'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for version issues (i.e. if you think your code is right, but you're not getting the right plot), try the following. In your terminal:\n",
    "\n",
    "1. Create a new environment with `python -m venv venv`\n",
    "2. Activate that environment with `source venv/bin/activate`\n",
    "3. Make sure the interpreter in the top right corner of your VSCode (or whatever u use to run ur code is venv).\n",
    "4. If you get a \"install kernel\" message, press it.\n",
    "5. Run `pip install -r requirements.txt`\n",
    "6. Run the remainder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from T3_P3_TestCases import test_forward_pass\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0) Why PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is an open source machine learning framework based on the Torch library. It is currently by far the most popular library for implementing machine learning projects. Unlike in NumPy, PyTorch allows you to perform tensor operations on the GPU. This, alongside dynamic auto-differentiation, makes PyTorch extremely performative and flexible. Learning to use PyTorch will mean you are able to prototype, train, and test production-quality models for work or research.\n",
    "\n",
    "All the building blocks for machine learning architectures (activation functions, loss functions, neural network layers, etc) are written for you. Exploring everything PyTorch offers takes patience. If you want to and dive further into deep learning, we highly encourage you to read through the [D2L (d2l.ai)](https://d2l.ai/) course, which gives tutorials on how to write even the most state of the art deep learning models from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1) Initializing tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing tensors in PyTorch is analagous to initializing arrays in NumPy. (This is because `torch.Tensor` is a wrapper of `numpy.ndarray`). Below we initialize a tensor with all $0$'s, one with all $1$'s, and one with all random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = 3, 2\n",
    "\n",
    "zeroes = torch.zeros(size=(n, d))\n",
    "print(f'Zeroes:\\n{zeroes}')\n",
    "\n",
    "ones = torch.ones(size=(n, d))\n",
    "print(f'\\nOnes:\\n{ones}')\n",
    "\n",
    "rand = torch.randn(size=(n, d))\n",
    "print(f'\\nRandom:\\n{rand}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `torch.randn` initializes a tensor according to some unknown random distribution. Let's sleuthe out what that distribution might be! First, we can sample this distribution by randomly initializing `n` many $48 \\times 64$ matrices (let's call them \"images\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 48, 64\n",
    "images = torch.randn(size=(n, height, width))\n",
    "\n",
    "print(f'Images shape:\\t\\t{images.shape}')\n",
    "print(f'First image shape:\\t{images[0].shape}')\n",
    "print(f'Second image shape:\\t{images[1].shape}')\n",
    "print(f'Third image shape:\\t{images[2].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape of `images` is `(3, 48, 64)`, meaning $3\\times 48 \\times 64$. This tensor has 3 axes, or dimensions. The first dimension indexes over all the individual images `images[0]`, `images[1]`, `images[2]`. The second dimension indexes over all the rows in a particular image. The third dimension indexes over the columns in a particular row. This means `images[0][5][2]`, or more cleanly `images[0,5,2]`, gives us the 2nd pixel on the 5th row of the 0th image.\n",
    "\n",
    "Now that we understand the shape of this `images` object we created, we can visualize the distribution of `torch.randn` by plotting a histogram of the pixel values for each image. To do this, let's flatten our collection of images by flattening the last two dimensions. Since each image is $48 \\times 64$, it has $3072$ pixels. Hence, we should see the result `flattened_images` to have the shape `(3, 3072)` and we do.\n",
    "\n",
    "Interesting! We can visually tell that `torch.randn` samples from the standard normal distribution $\\mathcal N(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_images = images.flatten(start_dim=1) # or images.reshape(n, -1)\n",
    "print(f'Flattened images shape:\\t{flattened_images.shape}')\n",
    "\n",
    "plt.title(f'Distribution of our {n} images')\n",
    "plt.hist(x=flattened_images, color=('crimson','purple','tomato'))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2) Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical operations in PyTorch are almost identical to that in NumPy. However, PyTorch can automatically differentiate operations by backpropogating through a computational graph dynamically produced behind the scenes. This computational graph keeps tracks of the calculations a tensor was part of. It is our responsibility, however, to explicitly tell PyTorch\n",
    "1. which tensors require gradients,\n",
    "2. and when to backpropogate to produce a gradient.\n",
    "\n",
    "For 1. we just need to pass the `requires_grad=True` argument into any of the initialization methods above. However, the `torch.nn.Parameter` class will do this for you for tensors that already exist. We will use this wrapper because it is meant for tensors that represent neural network parameters which will be updated after backpropogation.\n",
    "\n",
    "For 2. we simply call the `.backward()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3       # define  f(x) = x^3\n",
    "df = lambda x: 3 * x**2  # define âˆ‡f(x) = 3*x^2 as sanity check\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)  # initialize x = 1.0\n",
    "x = torch.nn.Parameter(torch.tensor(1.0))  # this also initializes x = 1.0\n",
    "\n",
    "y = f(x)      # calculate f(x)\n",
    "y.backward()  # backpropogate and calculate gradient of f(x) at x = 1.0\n",
    "print(f'Gradient of f(x) at x = {x}:\\t{x.grad}')\n",
    "print(f'Is this gradient right?:\\t{x.grad == df(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even plot the function's derivative over a range to visualize it! Note that before passing our tensors into Matplotlib's `plot`, we must detach them from their gradient information. That is, we want only the underlying tensor. We can do this by `with torch.no_grad()`. If we do not do this, PyTorch will panic since Matplotlib will try to convert the tensor to a NumPy array but cannot because that tensor requires a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(start=-2, end=2, steps=101, requires_grad=True)  # points spread across x-axis\n",
    "\n",
    "y = f(x)            # calculate y for every point on x-axis\n",
    "y.sum().backward()  # collapse y into single value in order to calculate the gradient at x value in parrelel\n",
    "\n",
    "with torch.no_grad():  # specify that we will not want to backpropogate on operations within this block\n",
    "  plt.plot(x, y, label='$f(x)$', c='k')\n",
    "  plt.plot(x, x.grad, label=r'$\\nabla f(x)$', c='tomato')\n",
    "plt.title('Plotting a function and its derivative')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, `with torch.no_grad()` is very handy for using our network for prediction or once training is done.\n",
    "\n",
    "We can also zero out gradients. This is necessary to reset gradients after every round of gradient descent. (Pay attention, because you will be implementing gradient descent). Notice what happens if we don't zero out our gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(f'Gradient of f(x) at x = {x}:\\t{x.grad}')\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(f'Gradient of f(x) at x = {x}:\\t{x.grad} (wrong)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice what happens if we do zero out our gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(f'Gradient of f(x) at x = {x}:\\t{x.grad}')\n",
    "\n",
    "x.grad.zero_()\n",
    "print(f'Gradient after zero=ing:\\t{x.grad}')\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(f'Gradient of f(x) at x = {x}:\\t{x.grad} (right)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3) MNIST Dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is an extremely popular (and non-trivial) example dataset. It consists of $28 \\times 28$ black-and-white images of handwritten single digits between 0 and 9. Our goal is to implement and train an MLP to classify these images from scratch.\n",
    "\n",
    "All the code in this part is written for you, but you should follow each line as you likely need to write these exact (or similar) lines of code multiple times in future research or work.\n",
    "\n",
    "We define a function below that\n",
    "1. downloads the MNIST training and test datasets,\n",
    "2. then wraps each dataset in a \"dataloader\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size):\n",
    "  trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "  testset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "  iter_from = lambda dataset: torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, num_workers=2)\n",
    "  return iter_from(trainset), iter_from(testset), trainset, testset\n",
    "\n",
    "mnist_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how both the dataset and an individual labelled datapoint looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter, train_set, test_set = load_mnist(batch_size=4)\n",
    "print(f'Length of training dataset:\\t{len(train_set)}')\n",
    "print(f'Length of test dataset:\\t\\t{len(test_set)}')\n",
    "\n",
    "x1, y1 = train_set[0]\n",
    "y1_text = mnist_labels[y1]\n",
    "print(f'First datapoint (x_1, y_1):\\t(Image: shape {tuple(x1.shape)}, Label: {y1} ({y1_text})),')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each image is $1\\times28\\times28$: this first dimension represents the different \"channels\" of the image. In this case, each image is black and white so there is only $1$ channel. However, in other datasets with colored images, we would see red, green, and blue channels: $3\\times28\\times28$. Another important thing to notice is that the labels are integers $$y_n \\in \\{0, \\ldots, 9\\}$$ where $y_n = i$ means the $n$-th image in the dataset corresponds to the clothing item `mnist_labels[i]`.\n",
    "\n",
    "What is the difference between a dataset and a dataloader? Recall that in a single epoch of stochastic gradient descent, we don't calculate loss on the entire dataset. Instead, within any particular epoch, we iterate over the entire dataset in batches. Dataloaders automatically shuffle and partition the dataset in such a way we can loop over them conviniently in every epoch. Let's look at a single mini batch of size $4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(X, y):\n",
    "  batch_size = X.shape[0]\n",
    "  _, axs = plt.subplots(1, batch_size, figsize=(batch_size*2, 2))\n",
    "  for ax, img, y in zip(axs, X, y):\n",
    "    ax.imshow(img[0], cmap='binary', interpolation='none')\n",
    "    ax.set_title(mnist_labels[y]) \n",
    "    ax.axis('off')\n",
    "\n",
    "  \n",
    "for X, y in train_iter:  # this is how we iterate over the entire training dataset\n",
    "  batch_shape = X.shape\n",
    "  print(f'Shape of batch X:\\t {batch_shape}')\n",
    "\n",
    "  data_shape = X[0].shape\n",
    "  print(f'Shape of datapoint x_i:\\t {data_shape}')\n",
    "\n",
    "  plot_examples(X, y)\n",
    "  break  # break since it would take 60000/4 = 15000 iterations to finish iterating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4) Creating a neural network**\n",
    "\n",
    "It is now time to implement a multilayer perceptron (MLP) with the following architecture:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathbf H &= \\operatorname{ReLU}(\\mathbf X \\mathbf W^{(1)} + \\mathbf b^{(1)}) \\\\\n",
    "  \\mathbf O &= \\operatorname{softmax}(\\mathbf H \\mathbf W^{(2)} + \\mathbf b^{(2)}).\n",
    "\\end{align*}\n",
    "\n",
    "The matrix $\\mathbf X \\in \\mathbb R^{M \\times D}$ above corresponds to a minibatch of $M$ datapoints $\\mathbf x_i \\in \\mathbb R^D$. The matrix $\\mathbf H \\in \\mathbb R^{M \\times J}$ is the hidden layer. For this problem, we choose the width of the hidden layer to be $J = 256$. The $i$-th row of the output matrix $\\mathbf O \\in \\mathbb R^{M \\times K}$ corresponds to the prediction $\\hat{\\mathbf y} \\in \\mathbb R^K$ of the $i$-th datapoint. We one hot encode the labels and use a softmax on the last layer so that our MLP models a proper distribution $p(y_n|\\mathbf x_n)$. Given an image $\\mathbf x^*$, if the $k$-th entry of our output $\\hat{\\mathbf y}^*$ is the largest, then we predict class $k$. This corresponds to picking the class $y^*$ that maximizes $p(y^* | \\mathbf x)$.\n",
    "\n",
    "![](mlp.svg)\n",
    "\n",
    "The entirety of implementing this architecture is just implementing the equations above. In order to do this, you will need to\n",
    "1. **TODO:** initialize your parameters $\\mathbf W^{(1)}, \\mathbf b^{(1)}, \\mathbf W^{(2)}, \\mathbf b^{(2)}$\n",
    "2. **TODO:** implement your activation functions $\\operatorname{ReLU}$, $\\operatorname{softmax}$\n",
    "3. **TODO:** implement your forward pass $f(\\mathbf X) = \\ldots$\n",
    "\n",
    "Machine learning models can be sensitive to weight initialization. For this architecture, initialize the weights as follows:\n",
    "\\begin{align*}\n",
    "  \\mathbf {W^{(\\ell)}}_{i,k} &\\sim \\mathcal 0.01 \\, N(0, 1), \\\\\n",
    "  \\mathbf {b^{(\\ell)}}_{i}\\;\\,\\, &= 0\\,.\n",
    "\\end{align*}\n",
    "\n",
    "*Hint: we just learned initialize random tensors.*\n",
    "\n",
    "*Double hint: we are flattening each image before feeding it into the MLP. How many inputs nodes in our MLP does this mean?*\n",
    "\n",
    "*Triple hint: we are one hot encoding our labels. How many dimensions will each label have if there are 10 classes in MNIST?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We are given images of dimension 28 by 28 and we want to classify them as one of 10 classes (digits 0-9).\n",
    "At each step, we will be applying a linear transformation (W's) followed by an additive bias term (b's).\n",
    "We then apply a non-linearity (relu or softmax), which you will be implementing in the cell below.\n",
    "\"\"\"\n",
    "\n",
    "n_inputs = 'not implemented'\n",
    "n_hiddens = 'not implemented'\n",
    "n_outputs = 'not implemented'\n",
    "\n",
    "W1 = 'not implemented' # matrix applying linear transformation\n",
    "b1 = 'not implemented' # additive bias term\n",
    "W2 = 'not implemented' # matrix applying linear transformation\n",
    "b2 = 'not implemented' # additive bias term\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are just as much an aspect of architecture as the number of hidden layers or the network width. Implement\n",
    "1. **TODO:** the ReLU function: $$\\operatorname{ReLU}(x) = \\max(0, x)$$\n",
    "2. **TODO:** the softmax function: $$\\operatorname{softmax}(\\mathbf X)_{ij} = \\frac{\\exp(\\mathbf X_{ij})}{\\sum_k \\exp \\mathbf X_{ik})}.$$\n",
    "\n",
    "*Hint: See `torch.clamp`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  \"\"\"\n",
    "  For each element, set 0 as minimum value and nothing as maximum value. S\n",
    "  hould not change shape of x.\n",
    "\n",
    "  :param x: a 2D numpy array of output of first transformation. Shape is (batch_size, n_hiddens)\n",
    "  :return: a 2D numpy array of output of first non-linearity. Shape is (batch_size, n_hiddens)\n",
    "  \"\"\"\n",
    "  \n",
    "  'not implemented'\n",
    "\n",
    "def softmax(X):\n",
    "  \"\"\"\n",
    "  For each row, divide each exponentiated element by the sum of all exponentiated elements in that row. \n",
    "  Should not change shape of X.\n",
    "\n",
    "  :param X: a 2D numpy array of output of second transformation. Shape is (batch_size, n_outputs)\n",
    "  :return: a 2D numpy array of output of second non-linearity. Shape is (batch_size, n_outputs)\n",
    "  \"\"\"\n",
    "\n",
    "  'not implemented'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to throw everything together!\n",
    "\n",
    "1. **TODO:** implement the forward pass.\n",
    "\n",
    "*Hint: The first dimension of `X` indexes over individual images in the batch. We want to flatten the rest of the dimensions.*\n",
    "\n",
    "*Double hint: Calculate $H$, then Calculate $O$.*\n",
    "\n",
    "*Triple hint: Like in NumPy, use `@` for matrix multiplication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "  \"\"\"\n",
    "  Implements the forward pass of our neural network. \n",
    "  \n",
    "  :param X: a batch of images of shape (batch_size, 1, 28, 28).\n",
    "  :return: a 2D numpy array of shape (batch_size, 10) where each row is a probability vector of length 10.\n",
    "  \"\"\"\n",
    "  \n",
    "  'not implemented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward_pass(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5) Training a neural network**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to evaluate our model prediction $\\hat {\\mathbf y}$ with the true label $\\mathbf y$. We can do this through with cross entropy loss. If $K$ is the number of categories we have, then $\\hat {\\mathbf y}$ and $\\mathbf y$ are $K$-dimensional one hot vectors. With this representation, cross entropy loss is defined as $$\\ell(\\hat {\\mathbf y}, \\mathbf y) = -\\sum_{k=1}^K \\mathbf y_i \\log \\hat {\\mathbf y}_i.$$ Note that the way we structured our neural network, only $\\hat {\\mathbf y}$ is one hot encoded, and the labels $y$ coming from the MNIST training set are integers. This means we can much more simply write $$\\ell(\\hat {\\mathbf y}, y) = -\\log \\hat {\\mathbf y}_y.$$\n",
    "\n",
    "For any general loss function, we can implement the gradient descent update step: $$w \\leftarrow w - \\eta \\, \\nabla_w \\mathcal L$$ for every model parameter $w$ where $\\mathcal L$ is the average loss over the minibatch.\n",
    "\n",
    "1. **TODO:** Implement cross entropy loss.\n",
    "2. **TODO:** Implement gradient descent.\n",
    "\n",
    "*Hint: in gradient descent, remember to use `with torch.no_grad():` since we don't want to backpropogate through the update operations.*\n",
    "\n",
    "*Double hint: assume that the parameters come into `sgd` with their gradients. Hence, $\\nabla_w \\mathcal L$ is simply `w.grad`.*\n",
    "\n",
    "*Triple hint: we will need to reset the parameter gradients by zero-ing out at end of gradient descent.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "  \"\"\"\n",
    "  Implement cross entropy loss as defined in the formula above.\n",
    "  :param y_hat: a 2D numpy array of shape (batch_size, 10) where each row is a probability vector of length 10.\n",
    "  :param y: a 1D numpy array of shape (batch_size) where each element is a label.\n",
    "\n",
    "  :return: a 1D numpy array of shape (batch_size) representing the cross entropy losses.\n",
    "  \"\"\"\n",
    "\n",
    "  'not implemented'\n",
    "\n",
    "def sgd(params, lr=0.1):\n",
    "  \"\"\"\n",
    "  Implements stochastic gradient descent. For each param, subtract the gradient times the learning rate.\n",
    "  The gradient for each param is stored in param.grad. After updating each param, set its gradient to zero.\n",
    "\n",
    "  :param params: a list of parameters to update.\n",
    "  :param lr: the learning rate.\n",
    "\n",
    "  :return: None\n",
    "  \"\"\"\n",
    "  \n",
    "  'not implemented'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have all the pieces to train a neural network... except the training loop:\n",
    "\n",
    "```\n",
    "  For every epoch:\n",
    "      For every batch X, y in training dataset:\n",
    "          1. Calculate y_hat from X\n",
    "          2. Calculate the losses between y_hat and y\n",
    "          3. Calculate total loss (the average over all losses)\n",
    "          4. Backpropagate on the loss\n",
    "          5. Update parameters with SGD\n",
    "```\n",
    "\n",
    "1. **TODO:** Implement the training loop.\n",
    "\n",
    "*Hint: Realize that the cross entropy loss we implemented defines loss for a single prediction. The loss over an entire minibatch of size $M$ is actually the average cross entropy loss.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, params, train_iter, loss_func=cross_entropy, updater=sgd):\n",
    "  \"\"\"\n",
    "  Implements training loop as described in the 5 steps above.\n",
    "\n",
    "  :param net: the neural network.\n",
    "  :param params: the parameters of the neural network.\n",
    "  :param train_iter: the training data iterator.\n",
    "  :param loss_func: the loss function.\n",
    "  :param updater: the function that updates the gradients.\n",
    "\n",
    "  :return: None\n",
    "  \"\"\"\n",
    "  \n",
    "  'not implemented'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished! Sure we can use the training loop above, but how will we know if we are succesfully trianing our model? Tracking training progress is crucial in understanding if changes ought to be made regarding hyperparameters or model architecture. Because of this, we expand the above training loop by adding a few lines that track various metrics such as\n",
    "1. average train loss\n",
    "2. average train accuracy\n",
    "3. average test loss\n",
    "4. average test accuracy\n",
    "\n",
    "Note that while this looks much scarier, it can be completely reduced to the much smaller trianing loop above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "train_iter, test_iter, _, _ = load_mnist(batch_size=batch_size)\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "  with torch.no_grad():\n",
    "    y_labels = y_hat.argmax(axis=1)\n",
    "    correct = y_labels == y\n",
    "    return correct.sum() / correct.numel()\n",
    "\n",
    "\n",
    "def train(net, params, train_iter, test_iter, loss, updater):\n",
    "  train_losses, train_accs = [], []\n",
    "  test_losses, test_accs = [], []\n",
    "  \n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    trials = 0\n",
    "\n",
    "    for X, y in train_iter:\n",
    "      trials += 1\n",
    "      y_hat = net(X)\n",
    "      l = loss(y_hat, y).mean()\n",
    "      acc = accuracy(y_hat, y)\n",
    "\n",
    "      l.backward()\n",
    "      updater(params, lr)\n",
    "\n",
    "      train_loss += l\n",
    "      train_acc += acc\n",
    "\n",
    "    train_losses.append(train_loss.item() / trials)\n",
    "    train_accs.append(train_acc.item() / trials)\n",
    "\n",
    "    test_loss, test_acc = 0.0, 0.0\n",
    "    trials = 0\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for X, y in test_iter:\n",
    "      trials += 1\n",
    "      with torch.no_grad():\n",
    "        y_hat = net(X)\n",
    "\n",
    "        # Recording for confusion matrix\n",
    "        if epoch == epochs-1:\n",
    "          y_pred += y_hat.argmax(axis=1)\n",
    "          y_true += y\n",
    "        \n",
    "        l = loss(y_hat, y).mean()\n",
    "        acc = accuracy(y_hat, y)\n",
    "\n",
    "        test_loss += l\n",
    "        test_acc += acc\n",
    "    \n",
    "    test_losses.append(test_loss.item() / trials)\n",
    "    test_accs.append(test_acc.item() / trials)\n",
    "  \n",
    "  return train_losses, train_accs, test_losses, test_accs, y_pred, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the fruit of our labor, we can train our from-scratch MLP and plot the training metrics. Great work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, test_losses, test_accs, y_pred, y_true = train(net, params, train_iter, test_iter, loss=cross_entropy, updater=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, c='crimson', label='train loss')\n",
    "plt.plot(train_accs, '--', c='crimson', label='train acc')\n",
    "plt.plot(test_losses, c='navy', label='test loss')\n",
    "plt.plot(test_accs, '--', c='navy', label='test acc')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, epochs-1)\n",
    "plt.legend()\n",
    "plt.savefig('final_plot.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in mnist_labels],\n",
    "                     columns = [i for i in mnist_labels])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Distribution (OOD) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "testset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing our train data to be full of just 1's and 6's\n",
    "new_train_set = []\n",
    "for X, y in trainset:\n",
    "    if y == 1 or y == 6:\n",
    "        new_train_set.append((X, y))\n",
    "\n",
    "# Constructing our in-distribution test data to be full of just 1's and 6's\n",
    "in_test_set = []\n",
    "for X, y in testset:\n",
    "    if y == 1 or y == 6:\n",
    "        in_test_set.append((X, y))\n",
    "\n",
    "# Constructing our out-of-distribution test data to be full of just 3's\n",
    "out_test_set = []\n",
    "for X, y in testset:\n",
    "    if y == 3:\n",
    "        out_test_set.append((X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_from = lambda dataset: torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "train_iter = iter_from(new_train_set)\n",
    "in_test_iter = iter_from(in_test_set)\n",
    "out_test_iter = iter_from(out_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's re-initialize our parameters. Note that this is the same code as what you have in part 4.1 in this notebook.\"\"\"\n",
    "\n",
    "W1 = 'not implemented' # matrix applying linear transformation\n",
    "b1 = 'not implemented' # additive bias term\n",
    "W2 = 'not implemented' # matrix applying linear transformation\n",
    "b2 = 'not implemented' # additive bias term\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_train_test_function(net, params, train_iter, in_test, out_test, loss, updater):\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    trials = 0\n",
    "\n",
    "    for X, y in train_iter:\n",
    "      trials += 1\n",
    "      y_hat = net(X)\n",
    "      l = loss(y_hat, y).mean()\n",
    "\n",
    "      l.backward()\n",
    "      updater(params, lr)\n",
    "  \n",
    "  # In distribution\n",
    "  test_acc = 0.0\n",
    "  trials = 0\n",
    "  for X, y in in_test:\n",
    "    trials += 1\n",
    "    with torch.no_grad():\n",
    "      y_hat = net(X)\n",
    "      l = loss(y_hat, y).mean()\n",
    "      acc = accuracy(y_hat, y)\n",
    "      test_acc += acc\n",
    "  \n",
    "  print(\"Test Accuracy (In Distribution): \" + str(test_acc.item() / trials))\n",
    "  \n",
    "  # Out of distribution\n",
    "  test_acc = 0.0\n",
    "  trials = 0\n",
    "  for X, y in out_test:\n",
    "    trials += 1\n",
    "    with torch.no_grad():\n",
    "      y_hat = net(X)\n",
    "      l = loss(y_hat, y).mean()\n",
    "      acc = accuracy(y_hat, y)\n",
    "\n",
    "      test_acc += acc\n",
    "  \n",
    "  print(\"Test Accuracy (Out of Distribution): \" + str(test_acc.item() / trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_test_function(net, params, train_iter, in_test_iter, out_test_iter, loss=cross_entropy, updater=sgd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "36362f484087c7d4513b440711cd7fad236e93840e2eb62be95f6c75e21a8eda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
